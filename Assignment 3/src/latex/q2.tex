\section*{Question 2}

\subsection*{(1)}

Given training data
\[
    \{(x_i, y_i)\}_{i=1}^n, \quad x_i \in \mathbb{R}^d,~ y_i \in \{+1, -1\}.
\]
Let
\[
    w \in \mathbb{R}^d, \quad b \in \mathbb{R}, \quad \xi = [\xi_1, \dots, \xi_n]^\top \in \mathbb{R}^n, \ \xi_i \ge 0.
\]

The soft-margin SVM primal optimization problem is
\[
    \begin{aligned}
        \min_{w, b, \xi} \quad & \frac{1}{2}\|w\|_2^2 + C \sum_{i=1}^n \xi_i            \\
        \text{s.t.} \quad      & y_i (w^\top x_i + b) \ge 1 - \xi_i, \quad i=1,\dots,n, \\
                               & \xi_i \ge 0, \quad i=1,\dots,n.
    \end{aligned}
\]

With \(C = 1\), this becomes
\[
    \min_{w,b,\xi} \ \frac{1}{2}\|w\|_2^2 + \sum_{i=1}^n \xi_i
    \quad
    \text{s.t. } y_i(w^\top x_i + b) \ge 1 - \xi_i,~ \xi_i \ge 0.
\]

\subsection*{(2)}

Introduce Lagrange multipliers
\[
    \begin{aligned}
        \alpha_i \ge 0    & \text{ for } y_i(w^\top x_i+b) \ge 1-\xi_i \\
        \quad \mu_i \ge 0 & \text{ for } \xi_i \ge 0.
    \end{aligned}
\]

The Lagrangian is
\[
    \mathcal{L}(w,b,\xi;\alpha,\mu)
    = \frac{1}{2}\|w\|_2^2 + \sum_{i=1}^n \xi_i
    - \sum_{i=1}^n \alpha_i \big(y_i(w^\top x_i+b)-1+\xi_i\big)
    - \sum_{i=1}^n \mu_i \xi_i.
\]

Stationarity conditions:

\[
    \begin{aligned}
         & \frac{\partial \mathcal{L}}{\partial w}=0 \Rightarrow w = \sum_{i=1}^n \alpha_i y_i x_i,                             \\
         & \frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0,                                 \\
         & \frac{\partial \mathcal{L}}{\partial \xi_i}=0 \Rightarrow 1 - \alpha_i - \mu_i = 0 \Rightarrow \alpha_i + \mu_i = 1.
    \end{aligned}
\]

Since \(\mu_i \ge 0\), we have
\[
    0 \le \alpha_i \le 1 \quad (\text{generally } 0 \le \alpha_i \le C).
\]

Substitute back:
\[
    \begin{aligned}
        \mathcal{L}
         & = \frac{1}{2}\|w\|_2^2 - \sum_{i=1}^n \alpha_i y_i w^\top x_i + \sum_{i=1}^n \alpha_i \\
         & = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n
        \alpha_i \alpha_j y_i y_j x_i^\top x_j
        + \sum_{i=1}^n \alpha_i,
    \end{aligned}
\]
% where we used \(w = \sum_i \alpha_i y_i x_i\) and \(\sum_i \alpha_i y_i = 0\).

Dual problem:
\[
    \begin{aligned}
        \max_{\alpha}\quad
         & \sum_{i=1}^n \alpha_i
        - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^\top x_j, \\
        \text{s.t.}\quad
         & \sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \le \alpha_i \le 1.
    \end{aligned}
\]

% Matrix form:
% \[
%     \max_{\alpha} \quad \mathbf{1}^\top \alpha - \frac{1}{2}\alpha^\top Q \alpha,
%     \quad
%     \text{s.t. } y^\top \alpha = 0,\ 0 \le \alpha \le \mathbf{1},
% \]
% where \(Q_{ij} = y_i y_j x_i^\top x_j.\)

KKT conditions:
\[
    \begin{cases}
        w = \sum_i \alpha_i y_i x_i,                          \\
        \sum_i \alpha_i y_i = 0,                              \\
        0 \le \alpha_i \le 1,                                 \\
        y_i(w^\top x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0, \\
        \alpha_i [y_i(w^\top x_i + b) - 1 + \xi_i] = 0,       \\
        (1 - \alpha_i)\xi_i = 0.
    \end{cases}
\]

\subsection*{(3)}

At optimality,
\[
    w^\star = \sum_{i=1}^n \alpha_i^\star y_i x_i.
\]

For any support vector with \(0 < \alpha_i^\star < 1\),
\[
    y_i\big((w^\star)^\top x_i + b^\star\big) = 1.
\]
Thus,
\[
    b^\star = y_i - (w^\star)^\top x_i.
\]
The decision function is
\[
    f(x) = \operatorname{sign}\big((w^\star)^\top x + b^\star\big),
    \quad
    \text{and the decision boundary is } (w^\star)^\top x + b^\star = 0.
\]

\subsection*{(4)}

In the Lagrangian, \(\xi_i\) appears only linearly:
\[
    \sum_i \xi_i - \sum_i \alpha_i \xi_i - \sum_i \mu_i \xi_i
    = \sum_i \xi_i(1 - \alpha_i - \mu_i).
\]
Setting the derivative w.r.t.\ \(\xi_i\) to zero gives \(1 - \alpha_i - \mu_i = 0\), or \(\alpha_i + \mu_i = 1\).

Substituting this back eliminates all terms involving \(\xi_i\),
so \(\xi_i\) no longer appears in the dual function.

Its only effect is to constrain \(\alpha_i \le C (=1)\),
which forms the box constraint \(0 \le \alpha_i \le C\).

% \subsection*{Final summary}

% \[
%     \boxed{
%         \begin{aligned}
%             \text{Primal: }   &
%             \min_{w,b,\xi}\ \frac{1}{2}\|w\|^2 + \sum_i \xi_i,                                            \\
%                               & y_i(w^\top x_i + b) \ge 1 - \xi_i,\ \xi_i \ge 0.                          \\[3pt]
%             \text{Dual: }     &
%             \max_{\alpha}\ \sum_i \alpha_i - \tfrac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j x_i^\top x_j, \\
%                               & \sum_i \alpha_i y_i = 0,\quad 0 \le \alpha_i \le 1.                       \\[3pt]
%             \text{Decision: } &
%             f(x)=\operatorname{sign}\big((w^\star)^\top x + b^\star\big),
%             \quad w^\star=\sum_i \alpha_i^\star y_i x_i.
%         \end{aligned}
%     }
% \]
